{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALGORITHM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Linear Regression Algorithm**\n",
    "\n",
    "### **Step 1: Initialize Parameters**\n",
    "\n",
    "1. **Input**:\n",
    "\n",
    "   - Feature matrix \\( X \\) (input data).\n",
    "   - Target vector \\( Y \\) (output data).\n",
    "   - Set initial parameters (weights) \\( \\theta \\) to random values.\n",
    "\n",
    "2. **Output**:\n",
    "   - Initial parameters \\( \\theta \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Add Bias Term**\n",
    "\n",
    "1. **Add a Bias Term**:\n",
    "\n",
    "   - Augment the feature matrix \\( X \\) by adding a column of ones to account for the intercept \\( \\theta_0 \\) (bias term).\n",
    "   - New feature matrix \\( X_b \\) includes this bias term.\n",
    "\n",
    "2. **Output**:\n",
    "   - Augmented feature matrix \\( X_b \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Define Hypothesis Function**\n",
    "\n",
    "1. **Define the Hypothesis**:\n",
    "\n",
    "   - The hypothesis function \\( h(X) \\) is defined as the linear combination of the input features and the parameters:\n",
    "     \\[\n",
    "     h(X) = X_b \\cdot \\theta\n",
    "     \\]\n",
    "   - This predicts the target value \\( \\hat{Y} \\) for a given input \\( X_b \\).\n",
    "\n",
    "2. **Output**:\n",
    "   - Predicted values \\( \\hat{Y} \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Define Cost Function**\n",
    "\n",
    "1. **Define the Cost Function**:\n",
    "\n",
    "   - Use the Mean Squared Error (MSE) as the cost function to measure the difference between the predicted values \\( \\hat{Y} \\) and the actual values \\( Y \\):\n",
    "     \\[\n",
    "     J(\\theta) = \\frac{1}{2m} \\sum\\_{i=1}^{m} (h(X^{(i)}) - Y^{(i)})^2\n",
    "     \\]\n",
    "   - \\( m \\) is the number of training examples.\n",
    "\n",
    "2. **Output**:\n",
    "   - The cost \\( J(\\theta) \\) representing the model’s error.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Implement Gradient Descent**\n",
    "\n",
    "1. **Iteratively Update Parameters**:\n",
    "\n",
    "   - For a given number of iterations, update the parameters \\( \\theta \\) using the gradient descent algorithm:\n",
    "     \\[\n",
    "     \\theta = \\theta - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "     \\]\n",
    "   - Here, \\( \\alpha \\) is the learning rate, and the gradient \\( \\frac{\\partial J(\\theta)}{\\partial \\theta} \\) is computed as:\n",
    "     \\[\n",
    "     \\text{Gradient} = \\frac{1}{m} X_b^T \\cdot (h(X) - Y)\n",
    "     \\]\n",
    "   - The gradient points in the direction of the steepest increase of the cost function, so subtracting it from \\( \\theta \\) moves the parameters towards the minimum of the cost function.\n",
    "\n",
    "2. **Output**:\n",
    "   - Optimized parameters \\( \\theta \\).\n",
    "   - A history of the cost function value at each iteration (optional).\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Train the Model**\n",
    "\n",
    "1. **Minimize the Cost Function**:\n",
    "\n",
    "   - Run gradient descent until the cost function converges or reaches a predefined number of iterations.\n",
    "   - The parameters \\( \\theta \\) that minimize the cost function are considered the optimal model parameters.\n",
    "\n",
    "2. **Output**:\n",
    "   - Final optimized parameters \\( \\theta \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 7: Make Predictions**\n",
    "\n",
    "1. **Predict New Values**:\n",
    "\n",
    "   - Use the optimized parameters \\( \\theta \\) to make predictions on new data:\n",
    "     \\[\n",
    "     \\hat{Y} = X_b \\cdot \\theta\n",
    "     \\]\n",
    "   - These predictions \\( \\hat{Y} \\) are the model’s outputs for the given inputs.\n",
    "\n",
    "2. **Output**:\n",
    "   - Predicted target values \\( \\hat{Y} \\) for the input features.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 8: Evaluate the Model**\n",
    "\n",
    "1. **Evaluate the Model’s Performance**:\n",
    "\n",
    "   - Assess the model by computing the final cost \\( J(\\theta) \\) using the optimized parameters.\n",
    "   - Optionally, use other metrics such as R-squared, Root Mean Squared Error (RMSE), etc.\n",
    "\n",
    "2. **Output**:\n",
    "   - Final cost value \\( J(\\theta) \\).\n",
    "   - Model parameters \\( \\theta \\) that can be used for further predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Algorithm**\n",
    "\n",
    "1. **Initialize Parameters**: Start with random weights \\( \\theta \\).\n",
    "2. **Add Bias Term**: Include a bias term in the feature matrix.\n",
    "3. **Define Hypothesis**: Create a linear model \\( h(X) = X_b \\cdot \\theta \\).\n",
    "4. **Define Cost Function**: Use Mean Squared Error to measure the model’s error.\n",
    "5. **Gradient Descent**: Iteratively adjust \\( \\theta \\) to minimize the cost function.\n",
    "6. **Train the Model**: Continue until the cost function converges.\n",
    "7. **Make Predictions**: Use the final \\( \\theta \\) to predict outcomes for new data.\n",
    "8. **Evaluate the Model**: Assess the performance using the final cost or other metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "Y = np.array([3,7,5,11,14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "theta = np.random.randn(X_b.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 2.],\n",
       "       [1., 3.],\n",
       "       [1., 4.],\n",
       "       [1., 5.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    return np.dot(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X, Y, theta):\n",
    "    m = len(Y)\n",
    "    predictions = predict(X, theta)\n",
    "    cost = (1 / (2*m)) * np.sum((predictions - Y.reshape(-1, 1)) ** 2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, Y, theta, learningRate, iterations):\n",
    "    m = len(Y)\n",
    "    costHistory = np.zeros(iterations)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        gradients = (1/m) * X.T.dot(predict(X, theta) - Y.reshape(-1,1))\n",
    "        theta = theta - learningRate * gradients\n",
    "        costHistory[i] = computeCost(X, Y, theta)\n",
    "        \n",
    "    return theta, costHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.01\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaOptimal, costHistory = gradientDescent(X_b, Y, theta, learningRate, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.94138102],\n",
       "       [ 5.48721863],\n",
       "       [ 8.03305624],\n",
       "       [10.57889385],\n",
       "       [13.12473147]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YPred = predict(X_b, thetaOptimal)\n",
    "YPred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.614914137571756"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalCost = computeCost(X_b, Y, thetaOptimal)\n",
    "finalCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3955434 ],\n",
       "       [2.54583761]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetaOptimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [[ 2.79504026]\n",
      " [ 5.39694031]\n",
      " [ 7.99884037]\n",
      " [10.60074042]\n",
      " [13.20264047]]\n",
      "Final Cost:  1.2400042825765334\n",
      "Theta Optimal [[0.19314021]\n",
      " [2.60190005]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "Y = np.array([3,7,5,11,14])\n",
    "\n",
    "X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "theta = np.random.randn(X_b.shape[1], 1)\n",
    "\n",
    "def predict(X, theta):\n",
    "    return np.dot(X, theta)\n",
    "\n",
    "def computeCost(X, Y, theta):\n",
    "    m = len(Y)\n",
    "    predictions = predict(X, theta)\n",
    "    cost = (1 / (2*m)) * np.sum((predictions - Y.reshape(-1, 1)) ** 2)\n",
    "    return cost\n",
    "\n",
    "def gradientDescent(X, Y, theta, learningRate, iterations):\n",
    "    m = len(Y)\n",
    "    costHistory = np.zeros(iterations)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        gradients = (1/m) * X.T.dot(predict(X, theta) - Y.reshape(-1,1))\n",
    "        theta = theta - learningRate * gradients\n",
    "        costHistory[i] = computeCost(X, Y, theta)\n",
    "        \n",
    "    return theta, costHistory\n",
    "\n",
    "learningRate = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "thetaOptimal, costHistory = gradientDescent(X_b, Y, theta, learningRate, iterations)\n",
    "\n",
    "YPred = predict(X_b, thetaOptimal)\n",
    "print(\"Predictions: \", YPred)\n",
    "\n",
    "finalCost = computeCost(X_b, Y, thetaOptimal)\n",
    "print(\"Final Cost: \", finalCost)\n",
    "\n",
    "print(\"Theta Optimal\", thetaOptimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
